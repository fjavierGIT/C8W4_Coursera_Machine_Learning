---
title: "Coursera Machine Learning Assignment"
author: "Javier Angoy"
date: "October 28, 2017"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
```

## Executive Summary 
The purpose of this exercise is to classificate the data from the Qualitative Activity Recognition of Weight Lifting Exercises study.  The study gathers data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants doing weight lifting exercises and tries to determine the movement performance (with 5 factor levels). So with this report our goal will be to predict the manner in which they did the exercise. This is the “classe” variable in the dataset.
More information on the “Weight Lifting Exercises Dataset” can be found on the following location:
http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

A comparison with three models was done: Linear Discriminant Annalysis, Boosting and Random Forests. The best precission achieved was that of the model Random Forest, with a 99.4%, under which we will try to identify 20 test cases with no "classe" data.

Github html version of this exercise can be found on [Coursera Machine Learning Assignment]( http://htmlpreview.github.io/?https://github.com/fjavierGIT/C8W4_Coursera_Machine_Learning/blob/master/index.html).

## Load libraries
```{r}
library(caret)
library(parallel)
library(doParallel)
```

## Loading and preprocessing the data
The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

First download data files. The dataset was loaded to R:
```{r, echo=TRUE, cache=TRUE}
if(!file.exists("./data")){
    dir.create("./data")
    fUrlTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    fUrlTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(fUrlTrain, destfile = "./data/TrainData.csv", method = "auto")
    download.file(fUrlTest, destfile = "./data/TestData.csv", method = "auto")
}
rawTrainingDF = read.csv("./data/TrainData.csv",na.strings = c("","NA"))
rawTestingDF = read.csv("./data/TestData.csv",na.strings = c("","NA"))
```

An exploration through the data set:
```{r}
head(rawTrainingDF)
```
Our dataset contains 160 variables, being the last column the "classe" variable we are trying to predict. 
We take a look at "classe", which results in a factor variable with 5 possible values.
```{r}
str(rawTrainingDF$classe)
```

## Feature extraction
Second we see that some columns contain a great number of invalid entries (NAs and DIV/0).
Watching columns with more invalid data, those seem to be the ones starting with "kurtosis_", "skewness_", "amplitude_", "max_", "min_ "var_", "avg_", and "stddev_". We will ignore them because they are not measurements from accelerometers but summarization data along with "timestamp", "new_window" and "num_window". We will also ignore columns named "X" and "user_name" which wont give information to our model.
```{r}
ignoredCols <- grep("kurtosis_|skewness_|amplitude_|max_|min_|var_|avg_|stddev_|X|user_name|timestamp|window", names(rawTrainingDF))
training <- rawTrainingDF[,-ignoredCols]
testing <- rawTestingDF[,-ignoredCols]
str(training)
```

## Data Processing for Cross Validation
We will evaluate our model accuracy through cross-validation. We have two datasets, rawTrainingDF and rawTestingDF. The latter (rawTestingDF) will be kept for grading purposes, so we will have to split the training data frame into two data sets in a 80%/20% proportion: training_CV (80%) and testing_CV (20%).
```{r}
set.seed(123)
inTrain <- createDataPartition(y = training$classe, p = 0.8, list = FALSE)
training_CV <- training[inTrain, ]
testing_CV <- training[-inTrain, ]
```

## Classification Algorithms Selection
Being this a classification problem, we will build 3 different prediction models using the training data. The outcome will be “classe” variable and the resting 52 features are predictors. Random Forest and Boosting with trees are the main methods to be compared as they are commonly used classification methods. Linear Discriminant Analysis is added as a simpler and quicker method.

We prepare trainControl function and set resampling default to "Cross Validation". We set k=10 as a widely used K-Fold crossvalidation parameter.
```{r}
fitControl <- trainControl(method = "cv", number = 10)
```

### Linear Discriminant Analysis (lda)
```{r}
ptm <- proc.time()
mod1 <- train(classe~., method='lda', data=training_CV, trControl = fitControl)
mod1
proc.time() - ptm
```

## Random Forest Predictor (rf) (Parallel Processing)
Parallel package in conjunction with the trainControl() function in caret were used to improve processing time of the execution of the train() function with the Random Forest model.
```{r}
ptm <- proc.time()
x <- training_CV[,-53]; y <- training_CV[,53]
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
fitControl2 <- trainControl(method = "cv",
                           number = 10,
                           allowParallel = TRUE)
mod2 <- train(x,y, method="rf",data=training_CV,trControl = fitControl2)
mod2
stopCluster(cluster)
registerDoSEQ()
proc.time() - ptm
```

### Boosted Predictor (gbm)
```{r}
ptm <- proc.time()
mod3 <- train(classe~.,method="gbm",data=training_CV,verbose=FALSE,trControl=fitControl) 
mod3
proc.time() - ptm
```

## Cross validation
Confusion matrices were created for the 3 models using the “caret” package.

### Linear Discriminant Analysis Accuracy
```{r}
confusionMatrix(testing_CV$classe, predict(mod1, testing_CV))$overall[1]
```

### Random Forest Predictor Accuracy
```{r}
confusionMatrix(testing_CV$classe, predict(mod2, testing_CV))$overall[1]
```

### Boosted Predictor Accuracy
```{r}
confusionMatrix(testing_CV$classe, predict(mod3, testing_CV))$overall[1]
```

### Expected out of sample error
We can calculate the expected out-of-sample error as 1 - accuracy for predictions made against the cross-validation set. An accuracy above 99.43% on our cross-validation data, ensures us an error that is estimated at 0.56%.

## Conclusions
The confusion matrices show that, having a great accuracy of 0.966, Boosting performance is not better than the one we obtained from Random Forest (0.994). The accuracy for the Random Forest model was 0.994 (95% CI: (0.9918, 0.9967)) compared to 0.966 (95% CI: (0.9599, 0.9715)) for Boosting. A simpler Linear Discriminant Analysis scored 0.699 (95% CI: (0.6851, 0.714)). Therefore, we choose Random Forest as the model to be used.

## Including Plots

### Feature plot
As part of the elementary data exploration, a search for correlated variables was done. Plot with "total" predictors.
```{r}
totalCols <- which(grepl("^total", colnames(training), ignore.case = F))
totals <- training[, totalCols]
featurePlot(x = totals, y = training$classe, pch = 19, main = "Total Features plot", 
    plot = "pairs")
```

### Principal Components plot
Relative importance of the resulting principal components of the selected model. 
```{r}
varImpPlot(mod2$finalModel, sort = TRUE, pch = 19, col = 1, cex = 1, 
    main = "Principal Components Gini Decrease")
```

### Random Forest plot
Plot with the selected classification model.
```{r}
plot(mod2, log = "y", lwd = 2, main = "Random forest accuracy", xlab = "Predictors", 
    ylab = "Accuracy")
```

## Prediction on Test Dataset
We will now try to classificate the 20 different test cases from the original test dataset using the described model, obtaining the following results:
```{r}
testPrediction <- predict(mod2, newdata = testing)
testPrediction
```