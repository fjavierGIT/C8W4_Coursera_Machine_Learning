---
title: "Coursera Machine Learning Assignment"
author: "Javier Angoy"
date: "26 de octubre de 2017"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
```
## Executive Summary 

The purpose of this exercise tries to classificate the data from the Qualitative Activity Recognition of Weight Lifting Exercises study.  The study gathers data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants doing weight lifting exercises and try to determinate the movement performance (better to worst in 5 levels).

The goal for this report will be predict the manner in which they did the exercise. This is the “classe” variable in the dataset.
More information on the “Weight Lifting Exercises Dataset” can be found in the following location:
http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

## Load libraries
```{r}
library(caret)
```
## Loading and preprocessing the data
The training data for this project are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
The test data are available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

First download data files. The dataset was loaded to R:
```{r, echo=TRUE, cache=TRUE}
if(!file.exists("./data")){
    dir.create("./data")
    fUrlTrain <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
    fUrlTest <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
    download.file(fUrlTrain, destfile = "./data/TrainData.csv", method = "auto")
    download.file(fUrlTest, destfile = "./data/TestData.csv", method = "auto")
}
rawTrainingDF = read.csv("./data/TrainData.csv",na.strings = c("","NA"))
rawTestingDF = read.csv("./data/TestData.csv",na.strings = c("","NA"))
```

An exploration through the data set:
```{r}
head(rawTrainingDF)
```
Our dataset contains 160 variables, being the last column the "classe" variable we are trying to predict. 
We take a look at "classe", which results in a factor variable with 5 possible values.
```{r}
str(rawTrainingDF$classe)
```
## Feature extraction
Second we see that some columns contain a great number of invalid entries (NAs and DIV/0).
Watching columns with more invalid data, those seem to be the ones starting with "kurtosis_", "skewness_", "amplitude_", "max_", "min_ "var_", "avg_", and "stddev_". We will ignore them because they are not measurements from accelerometers but summarization data along with "timestamp", "new_window" and "num_window". We will also ignore columns named "X" and "user_name" which wont give information to our model.
```{r}
ignoredCols <- grep("kurtosis_|skewness_|amplitude_|max_|min_|var_|avg_|stddev_|X|user_name|timestamp|window", names(rawTrainingDF))
training <- rawTrainingDF[,-ignoredCols]
testing <- rawTestingDF[,-ignoredCols]
str(training)
```

## Data Processing for Cross Validation
We will evaluate our model accuracy through cross-validation. We have two datasets, rawTrainingDF and rawTestingDF. The latter (rawTestingDF) will be kept for grading purposes, so we will have to split the training data frame into two data sets in a 80%/20% proportion: training_CV (80%) and testing_CV (20%).
```{r}
set.seed(123)
inTrain <- createDataPartition(y = training$classe, p = 0.8, list = FALSE)
training_CV <- training[inTrain, ]
testing_CV <- training[-inTrain, ]
```

## Classification Algorithms Selection
Being this a classification problem, we will build 3 different prediction models using the training data. The outcome will be “classe” variable and the resting 52 features are predictors. Random Forest and Boosting with trees are the main methods to be compared as they are commonly used classification methods. Linear Discriminant Analysis is added as a simpler and quicker method.

We prepare trainControl function and set resampling default to "Cross Validation". We set k=10 as a widely used K-Fold crossvalidation parameter.
```{r}
fitControl <- trainControl(method = "cv", number = 10)
```
### Linear Discriminant Analysis (lda)
```{r}
ptm <- proc.time()
mod1 <- train(classe~., method='lda', data=training_CV, trControl = fitControl)
mod1
proc.time() - ptm
```
##Random Forest Predictor (rf) (Parallel Processing)
```{r}
library(parallel)
library(doParallel)
ptm <- proc.time()
x <- training_CV[,-53]
y <- training_CV[,53]
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
fitControl2 <- trainControl(method = "cv",
                           number = 10,
                           allowParallel = TRUE)
mod2 <- train(x,y, method="rf",data=training_CV,trControl = fitControl2)
mod2
stopCluster(cluster)
registerDoSEQ()
proc.time() - ptm
```
###Boosted Predictor (gbm)
```{r}
ptm <- proc.time()
mod3 <- train(classe~.,method="gbm",data=training_CV,verbose=FALSE,trControl=fitControl) 
mod3
proc.time() - ptm
```
## Cross validation
Confusion matrices were created for the 3 models using the “caret” package.

###Linear Discriminant Analysis Accuracy
```{r}
confusionMatrix(testing_CV$classe, predict(mod1, testing_CV))$overall[1]
```
###Random Forest Predictor Accuracy
```{r}
confusionMatrix(testing_CV$classe, predict(mod2, testing_CV))$overall[1]
```
###Boosted Predictor Accuracy
```{r}
confusionMatrix(testing_CV$classe, predict(mod3, testing_CV))$overall[1]
```
We can see that, with an accuracy of 0.966, this model's performance is not better than the one we obtained from Random Forest (0.994). Therefore, we choose Random Forest as the model to be used.

### Expected out of sample error
We can calculate the expected out-of-sample error as 1 - accuracy for predictions made against the cross-validation set. An accuracy above 99.43% on our cross-validation data, ensures us an error that is estimated at 0.56%.
## Conclusions

blablabla

## Including Plots
### Feature plot
Bet
Plot with "Total" columns, searching for correlated variables.
```{r}
# plot features
total <- which(grepl("^total", colnames(training), ignore.case = F))
totalAccel <- training[, total]
featurePlot(x = totalAccel, y = training$classe, pch = 19, main = "Feature plot", 
    plot = "pairs")
```

### Principal Components plot
Relative importance of the resulting principal components of the selected model. 
```{r}
varImpPlot(mod2$finalModel, sort = TRUE, pch = 19, col = 1, cex = 1, 
    main = "Principal Components Gini Decrease")
```
### Random Forest plot
```{r}
plot(mod2, log = "y", lwd = 2, main = "Random forest accuracy", xlab = "Predictors", 
    ylab = "Accuracy")
```

## Prediction on Test Dataset
We will now try to predict the 20 different test cases from the original test dataset using the described prediction model, obtaining the following predictions:
```{r}
testPrediction <- predict(mod2, newdata = testing)
testPrediction
```